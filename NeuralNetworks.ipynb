{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAadPTXBS6vdQSe7RuvocO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humblefool1997/nnlj/blob/main/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Load Data\n",
        "*   Define PyToch Model\n",
        "*   Define Loss Function and Optimizers\n",
        "*   Run a Training Loop\n",
        "*   Evaluate the Model\n",
        "*   Make Predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ1SPOsm_SyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ysq8ainp5SaY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import urllib.request\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "url  = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "download_path = \"data.csv\"\n",
        "\n",
        "# Download the file\n",
        "urllib.request.urlretrieve(url, download_path)\n",
        "\n",
        "# Load the file using np.loadtxt\n",
        "data = np.loadtxt('data.csv',delimiter=',')"
      ],
      "metadata": {
        "id": "Pvy7tNu4LZzx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[:,0:8]\n",
        "Y = data[:,8]\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "Y = torch.tensor(Y,dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "# define the model\n",
        "class FOCAST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden1 = nn.Linear(8,12)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.hidden2 = nn.Linear(12,8)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.output = nn.Linear(8,1)\n",
        "    self.act_output = nn.Sigmoid()\n",
        "  def forward(self,x):\n",
        "    x = self.act1(self.hidden1(x))\n",
        "    x = self.act2(self.hidden2(x))\n",
        "    x = self.act_output(self.output(x))\n",
        "    return x\n",
        "\n",
        "model = FOCAST()\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dve8FCFLLs7o",
        "outputId": "cfe76599-685e-4fc4-97c8-449ff6d6ee3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOCAST(\n",
            "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (act_output): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "\n",
        "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "number_of_epochs = 100\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        XBatch = X[i:i+batch_size]\n",
        "        YBatch = Y[i:i+batch_size]\n",
        "        YPrediction = model(XBatch)\n",
        "        YPrediction = YPrediction.view(YBatch.size())  # Reshape output tensor if needed\n",
        "        loss = loss_fn(YPrediction, YBatch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "YPred = model(X)\n",
        "accuracy = (YPred.round() == Y).float().mean()\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Make class predictions with the model\n",
        "predictions = (model(X) > 0.5).int()\n",
        "for i in range(5):\n",
        "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], Y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baHu2RGP2VuP",
        "outputId": "6d41c302-224c-494c-f71d-5b422de49982"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7447916865348816\n",
            "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
            "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
            "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
            "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
            "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install onnx onnxscript onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cJSVxaeXhG2",
        "outputId": "59210646-2ac2-445b-e30c-5995ecdd9f5c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20240421)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.11.0)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_model = FOCAST()\n",
        "torch_input = torch.randn(1, 1, 12, 8)\n",
        "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23cGvH5qX1OV",
        "outputId": "8891c43f-f33a-49e6-970d-520a119fd4a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_program.save(\"FOCAST_NN.onnx\")"
      ],
      "metadata": {
        "id": "4J7PPELpejNv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "\n",
        "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
        "print(f\"Input length: {len(onnx_input)}\")\n",
        "print(f\"Sample input: {onnx_input}\")\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"./FOCAST_NN.onnx\", providers=['CPUExecutionProvider'])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
        "\n",
        "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb1hHCvVfEga",
        "outputId": "753a1b82-56d1-4a83-e1bb-0441088161cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input length: 1\n",
            "Sample input: (tensor([[[[-1.0975, -0.2460, -0.8397, -0.0299, -0.4566, -0.5399,  0.2287,\n",
            "            1.1354],\n",
            "          [ 0.2854, -1.6507,  1.3029, -0.9485, -0.3040,  0.5208, -1.4332,\n",
            "            0.1326],\n",
            "          [-0.4551, -2.6082, -0.4451,  0.3021, -2.0437,  0.4382, -0.9650,\n",
            "           -1.1040],\n",
            "          [-2.0635, -0.9618,  0.5340,  2.1871,  0.9119, -0.8013, -0.8848,\n",
            "            0.1226],\n",
            "          [ 0.5032,  1.4424,  0.8510,  0.2507, -1.6922,  0.6560, -0.9740,\n",
            "            1.5166],\n",
            "          [ 0.6723, -0.2180,  0.9865, -0.9899,  2.0724,  1.9553,  0.7595,\n",
            "           -1.7206],\n",
            "          [-0.8144,  0.0927, -2.0373, -0.6052,  0.3486, -0.5206, -0.3024,\n",
            "            3.3848],\n",
            "          [-0.4549, -0.7630, -0.8956, -0.8808, -0.3955,  1.6807,  0.6848,\n",
            "           -0.2610],\n",
            "          [ 1.3772,  0.6872, -1.0175,  0.9090, -0.8344,  1.0668, -0.7011,\n",
            "            0.6986],\n",
            "          [ 0.0605,  0.6856,  2.5146,  1.8676,  0.6236, -0.0886,  0.0898,\n",
            "            0.1780],\n",
            "          [ 0.0412, -0.0562, -0.3197,  0.1278,  0.2853,  0.4816, -1.2796,\n",
            "           -2.2566],\n",
            "          [ 0.4865, -0.3503, -0.8982, -0.1245,  0.7634,  0.5316, -0.1916,\n",
            "           -1.6380]]]]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_outputs = torch_model(torch_input)\n",
        "torch_outputs = onnx_program.adapt_torch_outputs_to_onnx(torch_outputs)\n",
        "\n",
        "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
        "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
        "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
        "\n",
        "print(\"PyTorch and ONNX Runtime output matched!\")\n",
        "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
        "print(f\"Sample output: {onnxruntime_outputs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbQ1i0c1fV7P",
        "outputId": "db14537a-e214-43a7-a972-da14f60cef53"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch and ONNX Runtime output matched!\n",
            "Output length: 1\n",
            "Sample output: [array([[[[0.55346644],\n",
            "         [0.63283217],\n",
            "         [0.6099308 ],\n",
            "         [0.53460914],\n",
            "         [0.5578151 ],\n",
            "         [0.60101694],\n",
            "         [0.5393855 ],\n",
            "         [0.58695143],\n",
            "         [0.5622459 ],\n",
            "         [0.5508343 ],\n",
            "         [0.5815853 ],\n",
            "         [0.56927437]]]], dtype=float32)]\n"
          ]
        }
      ]
    }
  ]
}